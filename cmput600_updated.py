# -*- coding: utf-8 -*-
"""CMPUT600_Updated

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1eE25-zObD3LyjqaqB2JlY-VurQwOG6

## README


**All list of homonyms and processed data is present in the following directory in Google Drive that is required to run the notebook:**
https://drive.google.com/drive/folders/10BSrE8nLbX46dxQIHzNv5wo_Fv-LqZn0?usp=sharing


**It would be advisable to save a copy of this folder to your own drive and then start from checkpoint 5 in this notebook to replicate the results.**

Different checkpoints are provided to help ease the process of replicating results.
**One can search for Checkpoint 5 in this notebook and start executing the cells.**


Whenever possible, comments are provided to enhance the understandability of the code. Make sure you have all the python packages installed to run the checkpoints.

Set up BERT model -> using an API given in the following link:
https://github.com/imgarylai/bert-embedding
"""

pip install --upgrade numpy==1.14.6

!pip install bert-embedding

from bert_embedding.bert import BertEmbedding

bert_model = BertEmbedding()

"""Importing the homonym list and SemCor data from Drive. Can then extract sentences that contain homonyms and get embddings for these sentences from BERT.

Use the cleaned up version of SemCor provided by Bradley.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

homonym_list = pd.read_csv('/content/drive/My Drive/CMPUT600_data/hk20-homonym-resource_homonyms_v01.tsv', delimiter='\t', header=None, names=['word', 'number', 'pos', 'meaning'])

semcor = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor.data.xml.ins', delimiter='\t', header=None, names=['sent_id', 'sent', 'word_num', 'un_id', 'lemma', 'pos_tag'])

"""Now I read in sense-key data and attach to SemCor"""

semcor_key = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor.gold.key.txt', sep=' ', names=['un_id', 'sense_key'], header=None)

semcor_key.head()

semcor_with_sensekey = pd.merge(semcor, semcor_key, how='inner', on='un_id')

semcor_vals = semcor_with_sensekey.copy()

semcor_with_sensekey['un_id'] = 'semcor.' + semcor_with_sensekey['un_id'].astype(str)

semcor_with_sensekey.head()

semcor_with_sensekey.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_with_sensekey.csv', index=False)

"""Inner join on 'un_id' from SemCor and gold.key saved as 'semcor_with_sensekeys.csv' in Drive.

From here I will need to remove homonymous words from the list that do not appear in the 'lemma' of Semcor and do the other way around too.
"""

semcor_with_sensekey = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor_with_sensekey.csv')

matching_lemmas = [i for i in range(len(semcor_with_sensekey)) if semcor_with_sensekey['lemma'].iloc[i] in list(homonym_list['word'])]

unique_homs = sorted(list(set(homonym_list['word'])))

# This operation takes a long time.
keepers = []
for hom in unique_homs:
  hom_space = str(' ' + hom + ' ')
  for i in range(0, len(semcor_with_sensekey)):
    if ((hom_space in semcor_with_sensekey['sent'][i]) and (hom == semcor_with_sensekey['lemma'][i])):
      keepers.append(i)
    else: 
      pass

keepers = sorted(keepers)

keepers[0:10]

len(semcor_with_sensekey)

# Subset semcor_with_sensekey by only lemmas that appear in homonym list and appear in the sentence associated with the lemma
semcor_true_lemmas = semcor_with_sensekey.iloc[keepers]

len(semcor_true_lemmas)

#Do the other way around.
matching_homs = [i for i in range(len(homonym_list)) if homonym_list['word'].iloc[i] in list(semcor_true_lemmas['lemma'])]

len(homonym_list)

match_homonyms = homonym_list.iloc[matching_homs]

len(match_homonyms)

# Sanity check that sets of match_homonyms['word'] and semcor_true_lemmas['lemma'] are the same
set(semcor_true_lemmas['lemma']).difference(set(match_homonyms['word']))

set(match_homonyms['word']).difference(set(semcor_true_lemmas['lemma']))

semcor_true_lemmas = semcor_true_lemmas.reset_index(drop=True)
match_homonyms = match_homonyms.reset_index(drop=True)

# Write my reduced homonyms and semcor to csv as a checkpoint
match_homonyms.to_csv('/content/drive/My Drive/CMPUT600_data/match_homonyms.csv', index=False)
semcor_true_lemmas.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_true_lemmas.csv', index=False)

"""Note that semcor_true_lemmas and match_homonyms are dataframes that contain sentences from semcor that have a homonym in them and have a lemma that matches a homonym from the list, and a list of homonyms that are present in both sentences and the lemma column in semcor, respectively

Create a "sense number" for semcor data points to match with the homonym mapping provided by Professor Kondrak.

Use the sense number for SemCor sentences for merging with the homonym mapping.
"""

import re

semcor_true_lemmas['sense_number'] = ""

for i in range(0,len(semcor_true_lemmas)):
  lemma = semcor_true_lemmas['lemma'].iloc[i]
  pos = semcor_true_lemmas['pos_tag'].iloc[i][0].lower()
  number = re.search(r'\d+', semcor_true_lemmas['sense_key'].iloc[i]).group()
  semcor_true_lemmas['sense_number'].iloc[i] = str(lemma + '#' + pos + '#' + number)

semcor_true_lemmas.head()

#Overwriting match_semcor csv with updated dataframe
semcor_true_lemmas.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_true_lemmas.csv', index=False)

match_homonyms = pd.read_csv('/content/drive/My Drive/CMPUT600_data/match_homonyms.csv')

"""Merge the homonym mapping with SemCor using the sense numbers."""

# Checkpoint 1
hom_res_map = pd.read_csv('/content/drive/My Drive/CMPUT600_data/hk20-homonym-resource_mappings_v01.tsv', sep='\t', header=None, names=['sense_number', 'hom_type'])

semcor_true_lemmas = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor_true_lemmas.csv')

semcor_true_lemmas.head()

hom_res_map.head()

len(semcor_true_lemmas)

len(hom_res_map)

# We do an inner merge to drop any rows that have sense numbers not found in both the hom_res_map and semcor_true_lemmas
semcor_true_mapped = pd.merge(semcor_true_lemmas, hom_res_map, how='inner', on='sense_number')

len(semcor_true_mapped)

semcor_true_mapped.head(10)

# Writing semcor_true_mapped to csv
semcor_true_mapped.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_true_mapped.csv', index=False)

# Checkpoint 2
#semcor_true_mapped = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor_true_mapped.csv')
#match_homonyms = pd.read_csv('/content/drive/My Drive/CMPUT600_data/match_homonyms.csv')

# This loop will collect a list of the homonyms that are not actually homonymous in SemCor
no_rep_homs = []
for lemma in match_homonyms['word']:
  nums = []
  for i in range(0, len(semcor_true_mapped)):
    if (semcor_true_mapped['lemma'].iloc[i] == lemma):
      nums.append(semcor_true_mapped['hom_type'].iloc[i])
  distinct_homs = list(set(nums))
  length = len(distinct_homs)
  if (length <= 1):
    no_rep_homs.append(lemma)
    no_rep_homs = list(set(no_rep_homs))

len(no_rep_homs)

all_homs = list(set(match_homonyms['word']))

len(all_homs)

# Collect a list of indices for lemmas that do not appear in no_rep_homs, i.e. lemmas that have multiple homonyms present in the dataset
keepers = []
for i in range(0, len(semcor_true_mapped)):
  if (semcor_true_mapped['lemma'][i] not in no_rep_homs):
    keepers.append(i)
  else:
    pass

keepers[0:10]

# This df contains all entries that correspond to a lemma that has multiple homonyms present
semcor_truehoms = semcor_true_mapped.iloc[keepers]

len(set(semcor_truehoms['lemma']))

#Writing to csv in case runtime resets
semcor_truehoms.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_truehoms.csv', index=False)

# Checkpoint 3
semcor_truehoms = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor_truehoms.csv')

semcor_truehoms['hom_id'] = ''
for i in range(0, len(semcor_truehoms)):
  un = str(str(semcor_truehoms['sense_number'].iloc[i]) + '#' + str(semcor_truehoms['hom_type'].iloc[i]))
  semcor_truehoms['hom_id'].iloc[i] = un

semcor_truehoms.head()

occurences = semcor_truehoms['hom_id'].value_counts()

len(occurences)

# This will eliminate rows that have a unique hom_id in the dataset. The idea is that there would be only one example of that 
# homonymous sense so it should be excluded for clustering purposes.
rep_homs = semcor_truehoms.groupby('hom_id').filter(lambda hom_id: len(hom_id) > 1)

len(set(rep_homs['lemma']))

rep_homs = rep_homs.reset_index(drop=True)

no_rep_homs = []
for lemma in rep_homs['lemma']:
  nums = []
  for i in range(0, len(rep_homs)):
    if (rep_homs['lemma'].iloc[i] == lemma):
      nums.append(rep_homs['hom_type'].iloc[i])
  distinct_homs = list(set(nums))
  length = len(distinct_homs)
  if (length <= 1):
    no_rep_homs.append(lemma)
    no_rep_homs = list(set(no_rep_homs))

len(no_rep_homs)

# Collect a list of indices for lemmas that do not appear in no_rep_homs, i.e. lemmas that have multiple homonyms present in the dataset
keepers = []
for i in range(0, len(rep_homs)):
  if (rep_homs['lemma'][i] not in no_rep_homs):
    keepers.append(i)
  else:
    pass

semcor_final = rep_homs.iloc[keepers]

semcor_final = semcor_final.reset_index(drop=True)

len(set(semcor_final['lemma']))

# Write semcor_final to csv. This contains entries in semcor that have a lemma present in the homonym list and have at least two entries with 
#multiple homonyms. There are about 37 distinct lemmas. 
# Checkpoint 4
semcor_final.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_final.csv', index=False)

remaining_homs = list(set(semcor_final['lemma']))

remaining_homs[0:8]

occurences = semcor_final['hom_id'].value_counts()

semcor_final = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor_final.csv')

semcor_final.head()

semcor_final['bert_sent'] = ""

# Create new column that will have sentences to feed into BERT (Using all the words in a sentence as the context!)
for i in range(0, len(semcor_final)):
  words = semcor_final['sent'][i].split()
  spot = semcor_final['word_num'][i]
  length = len(words)
  start = max(0, (spot - 10))
  end = min(length, (spot + 10))
  truncate = words[start:end]
  semcor_final['bert_sent'][i] = ' '.join(word for word in truncate)

semcor_final.head()

# Contains only the data from semcor - reason, semcor contains 37 homonyms after reducing the file size.
# Checkpoint 5
semcor_final.to_csv('/content/drive/My Drive/CMPUT600_data/semcor_final.csv', index=False)

# len(semcor_final[semcor_final['lemma'] == 'light' and semcor_final['hom_type'] == 400])

import pandas as pd
semcor_final = pd.read_csv('/content/drive/My Drive/CMPUT600_data/semcor_final.csv')

sentences = []
for i in range(0, len(semcor_final)):
  if (('mean' == semcor_final['lemma'][i]) and (' mean ' in semcor_final['bert_sent'][i])):
    sentences.append(i)

semcor_final[200:205] # mean is another example of a homonym for which SemCor has sentences containing two distinct lemmas.

# test_sents = rep_homs.iloc[sentences]
test_sents = semcor_final.iloc[sentences]

len(test_sents)

"""Collecting all the sentences for a target word under consideration and grouping them with the sense number of the target word. 
This is done to later average the contextual embeddings for a specific sense of the target word.
"""

senses = dict()
corr_homs = []
for row in test_sents.values:
  senses[row[6] + str(row[8])] = []

for row in test_sents.values:
  senses[row[6] + str(row[8])].append(row[10])
print(senses)

pip install --upgrade numpy

import numpy as np

# Embeddings for the word 'mean'. This can be changed to the word 'light'.
embs = dict()
for key in senses:
  temp_emb = bert_model(senses[key]) # Feed in all the sentences for a senses of light.
  h_embs = []
  for row in temp_emb:
    try:
      h_index = row[0].index('mean')
      h_embs.append(row[1][h_index]) # Append the embedding for the word 'light'.
    except ValueError:
      print(row[0])
  temp_list = list(map(sum, zip(*h_embs)))
  embs[key] = np.divide(temp_list, len(temp_list))
# print(embs)

# Extract the homonym types, i.e. 100, 200, 300, and 400.
hom_types = []
sense_keys_temp = []
avg_embs = []
for key in embs:
  hom_types.append(key[-3:])
  sense_keys_temp.append(key[:-3])
  avg_embs.append(embs[key])
# print(sense_keys_temp)
# print(hom_types)

# Create a new dataframe with all the data extracted in the previous few cells.
# avg_emb_dict = {'sense_key':sense_keys_temp, 'hom_type':hom_types, 'avg_emb':avg_embs}
avg_emb_zip = zip(sense_keys_temp, hom_types, avg_embs)
avg_emb_df = pd.DataFrame(avg_emb_zip, columns=['sense_key', 'hom_type', 'avg_emb'])

len(h_embs)

from sklearn.cluster import DBSCAN
from sklearn.cluster import MeanShift
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import pairwise_distances
from scipy.spatial.distance import cosine

bert_vecs = np.array(avg_embs)

len(bert_vecs)

cluster = MeanShift().fit(bert_vecs)
cluster.labels_

clustering = DBSCAN(eps=0.25, min_samples=2, metric='cosine').fit(bert_vecs)
clustering.labels_

ag_cluster = AgglomerativeClustering(n_clusters=None, distance_threshold=0.55).fit(bert_vecs)
ag_cluster.labels_

# relevant

from sklearn.manifold import TSNE
from sklearn.manifold import LocallyLinearEmbedding # Will be used for LLE, LTSA, Hessian LLE, and Modified LLE.
from sklearn.manifold import MDS
from sklearn.manifold import SpectralEmbedding
from sklearn.manifold import Isomap
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Get the projection of the n-dimensional contextual embeddings into a lower dimensional space using differnt dimensionality reduction techniques.



NOTE: In the report, the results for TSNE, PCA, and multidimensional scaling is provided because the other techniques do not provide exciting results.
"""

matrix = np.array(avg_embs)
# tsne = TSNE(n_components=2, perplexity=5.0, early_exaggeration=12.0, metric='cosine',  init='pca').fit_transform(matrix)
tsne = TSNE(n_components=2).fit_transform(matrix)
pca = PCA(n_components=2).fit_transform(matrix)
lle = LocallyLinearEmbedding().fit_transform(matrix)
mds = MDS(dissimilarity='euclidean').fit_transform(matrix)
isomap = Isomap().fit_transform(matrix)
spectral = SpectralEmbedding().fit_transform(matrix)

import numpy
import matplotlib.pyplot as plt
import seaborn as sns

tsne[0]

tsne_df = pd.DataFrame({'X': tsne[:, 0], 'Y': tsne[:, 1]})
pca_df = pd.DataFrame({'X': pca[:, 0], 'Y': pca[:, 1]})
lle_df = pd.DataFrame({'X': lle[:, 0], 'Y': lle[:, 1]})
mds_df = pd.DataFrame({'X': mds[:, 0], 'Y': mds[:, 1]})
isomap_df = pd.DataFrame({'X': isomap[:, 0], 'Y': isomap[:, 1]})
spectral_df = pd.DataFrame({'X': spectral[:, 0], 'Y': spectral[:, 1]})

pca_df.head()

# test_sents['sense_key'].head()

relevant = pd.concat([avg_emb_df['sense_key'], avg_emb_df['hom_type']], axis = 1)
relevant.reset_index(inplace=True,drop=True)
mds_df.reset_index(inplace=True, drop=True)
relevant = pd.concat([relevant, mds_df], axis=1)
labels = [ 0,  0,  0,  0,  0, -1, -1,  1,  1]
cluster_labels = pd.DataFrame(labels, columns=['labels'])
relevant = pd.concat([relevant, cluster_labels], axis=1)

relevant

"""The following series of cells contain the visualizations for the word 'light' and 'mean' as examples of the dimensionality reduction techniques together with the clustering results.


The homonymous word under consideration is mentioned in the first line of each cell.
"""

# For the word 'light'. Do not run this cell becuase the word under consideration might be different now. The plor will change depending on the word.
plt.title('Using LLE on averaged embeddings with Hierarchical clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using isomap on averaged embeddings with Hierarchical clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using mds on averaged embeddings with Hierarchical clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using T-SNE on averaged embeddings with MeanShift clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using PCA on averaged embeddings with MeanShift clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using MDS on averaged embeddings with MeanShift clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using T-SNE on averaged embeddings with DBScan')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using PCA on averaged embeddings with DBScan')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'light'.
plt.title('Using MDS on averaged embeddings with DBScan')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'mean'.
plt.title('Using PCA on averaged embeddings with Hierarchical Clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'mean'.
plt.title('Using MDS on averaged embeddings with Hierarchical Clustering')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'mean'.
plt.title('Using PCA on averaged embeddings with MeanShift')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'mean'.
plt.title('Using MDS on averaged embeddings with MeanShift')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'mean'.
plt.title('Using PCA on averaged embeddings with DBScan')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line

# For the word 'mean'.
plt.title('Using MDS on averaged embeddings with DBScan')
sns.scatterplot('X', # Horizontal axis
           'Y', # Vertical axis
           data=relevant, # Data source
            hue='labels', style='hom_type')
           #fit_reg=False) # Don't fix a regression line